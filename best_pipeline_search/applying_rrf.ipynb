{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mean Recall@k for a list of recommendations.\n",
    "    \"\"\"\n",
    "    recalls_at_k = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate Recall@k for each recommendation list\n",
    "        true_set = set(true)\n",
    "        k = min(k, len(pred))\n",
    "        relevant_count = sum(1 for item in pred[:k] if item in true_set)\n",
    "        recalls_at_k.append(relevant_count / len(true_set) if len(true_set) > 0 else 0)\n",
    "\n",
    "    # Calculate the mean Recall@k\n",
    "    mean_recall = sum(recalls_at_k) / len(recalls_at_k) if recalls_at_k else 0\n",
    "\n",
    "    return mean_recall\n",
    "\n",
    "def mean_inv_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean inverse rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = 1 / (pred.index(item) + 1)\n",
    "            except ValueError:\n",
    "                rank = 0  # If item not found, assign 0\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks) if ranks else 0\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean inverse ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else 0\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "def mean_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = pred.index(item) + 1\n",
    "            except ValueError:\n",
    "                rank = len(pred)  # If item not found, assign the length of the list\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks) if ranks else 0\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else 0\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "# for getting true labels and our predictions\n",
    "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
    "    \"\"\"\n",
    "    Get the true and predicted labels for the metrics calculation.\n",
    "    \"\"\"\n",
    "    # for i in recommendations_dict:\n",
    "    #     print(i, recommendations_dict[i])\n",
    "    #     break\n",
    "    \n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    not_in_citation_mapping = 0\n",
    "\n",
    "    # Iterate over the items in both dictionaries\n",
    "    for citing_id in recommendations_dict.keys():\n",
    "        # Check if the citing_id is present in both dictionaries\n",
    "        if citing_id in citing_to_cited_dict:\n",
    "            # If yes, append the recommended items from both dictionaries to the respective lists\n",
    "            true_labels.append(citing_to_cited_dict[citing_id])\n",
    "            predicted_labels.append(recommendations_dict[citing_id])\n",
    "        else:\n",
    "            print(citing_id, \"not in citation mapping\")\n",
    "            not_in_citation_mapping += 1\n",
    "\n",
    "    return true_labels, predicted_labels, not_in_citation_mapping\n",
    "\n",
    "# load embeddings\n",
    "def load_embeddings_and_ids(embedding_file, app_ids_file):\n",
    "    \"\"\"\n",
    "    Load the embeddings and application IDs from saved files\n",
    "    \"\"\"\n",
    "    print(f\"Loading embeddings from {embedding_file}\")\n",
    "    embeddings = torch.from_numpy(np.load(embedding_file))\n",
    "\n",
    "    print(f\"Loading app_ids from {app_ids_file}\")\n",
    "    with open(app_ids_file, 'r') as f:\n",
    "        app_ids = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(embeddings)} embeddings and {len(app_ids)} app_ids\")\n",
    "    return embeddings, app_ids\n",
    "\n",
    "# calculating cosine similarity:\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j] = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "def pytorch_cos_sim(a, b):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j] = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    return cos_sim(a, b)\n",
    "\n",
    "# for getting train alignments\n",
    "def citation_to_citing_to_cited_dict(citations):\n",
    "    \"\"\"\n",
    "    Put a citation mapping in a dict format\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the results\n",
    "    citing_to_cited_dict = {}\n",
    "\n",
    "    # Iterate over the items in the JSON list\n",
    "    for citation in citations:\n",
    "        # Check if the citing id already exists in the resulting dictionary\n",
    "        if citation[0] in citing_to_cited_dict:\n",
    "            # If the citing id exists, append the cited id to the existing list\n",
    "            citing_to_cited_dict[citation[0]].append(citation[2])\n",
    "        else:\n",
    "            # If the citing id doesn't exist, create a new list with the cited id for that citing id\n",
    "            citing_to_cited_dict[citation[0]] = [citation[2]]\n",
    "\n",
    "    return citing_to_cited_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipelines = [\n",
    "    \"PatentSBERTa_claims_claims_results.json\",\n",
    "    \"PatentSBERTa_TAC_TAC_results.json\",\n",
    "    \"PatentSBERTa_TAC_claims_results.json\",\n",
    "    \"PatentSBERTa_claims_TAC_results.json\",\n",
    "]\n",
    "\n",
    "base_dir = \"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/github/not_git/best_pipeline_search_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result = []\n",
    "for i in best_pipelines:\n",
    "    result = {}\n",
    "    with open(base_dir+i) as f:\n",
    "        data = json.load(f)\n",
    "    for j in data[\"segments\"].keys():\n",
    "        result[j] = list(data[\"segments\"][j].keys())\n",
    "    total_result.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7301541989947786\n",
      "38.81004977309326\n",
      "0.28334207200935674\n",
      "\n",
      "0.7993778363343581\n",
      "31.679234372712642\n",
      "0.32925124376224923\n",
      "\n",
      "0.7812692138779097\n",
      "33.58022983457766\n",
      "0.31723982522320626\n",
      "\n",
      "0.7181183818864979\n",
      "40.43379202654566\n",
      "0.26066176824771214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = \"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025\"\n",
    "CITATION_FILE = os.path.join(BASE_DIR, \"Citation_JSONs/Citation_Train.json\")\n",
    "\n",
    "with open(CITATION_FILE, 'r') as f:\n",
    "    citations = json.load(f)\n",
    "citing_to_cited_dict = citation_to_citing_to_cited_dict(citations)\n",
    "\n",
    "for i in total_result:\n",
    "    true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted(citing_to_cited_dict, i)\n",
    "    recall_at_k = mean_recall_at_k(true_labels, predicted_labels, k=100)\n",
    "    print(recall_at_k)\n",
    "    mean_rank = mean_ranking(true_labels, predicted_labels)\n",
    "    print(mean_rank)\n",
    "    mean_inv_rank = mean_inv_ranking(true_labels, predicted_labels)\n",
    "    print(mean_inv_rank)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample query: 3725294A1\n",
      "Top 5 documents after fusion: ['3315117A1', '3315172A1', '1920766B1', '2046268B1', '1729811B1']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def reciprocal_rank_fusion(ranked_lists, k=20):\n",
    "    \"\"\"\n",
    "    Implements Reciprocal Rank Fusion to combine multiple ranked lists.\n",
    "    \n",
    "    Args:\n",
    "        ranked_lists: List of dictionaries, where each dictionary maps queries to ranked lists of document IDs\n",
    "        k: Constant to mitigate the impact of high rankings (default: 60)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping queries to fused ranked lists\n",
    "    \"\"\"\n",
    "    fused_results = {}\n",
    "    \n",
    "    # Get all unique query IDs\n",
    "    all_queries = set()\n",
    "    for result_dict in ranked_lists:\n",
    "        all_queries.update(result_dict.keys())\n",
    "    \n",
    "    # Process each query\n",
    "    for query in all_queries:\n",
    "        # Dictionary to store RRF scores for each document\n",
    "        rrf_scores = defaultdict(float)\n",
    "        \n",
    "        # Calculate RRF scores for each document from each ranked list\n",
    "        for result_dict in ranked_lists:\n",
    "            if query in result_dict:\n",
    "                doc_list = result_dict[query]\n",
    "                \n",
    "                # Calculate RRF score based on rank position (1-indexed)\n",
    "                for rank, doc_id in enumerate(doc_list, start=1):\n",
    "                    rrf_scores[doc_id] += 1.0 / (k + rank)\n",
    "        \n",
    "        # Sort documents by RRF score in descending order\n",
    "        sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        fused_results[query] = [doc_id for doc_id, score in sorted_docs]\n",
    "    \n",
    "    return fused_results\n",
    "\n",
    "# Example usage with your code\n",
    "best_pipelines = [\n",
    "    \"PatentSBERTa_claims_claims_results.json\",\n",
    "    \"PatentSBERTa_TAC_TAC_results.json\",\n",
    "    \"PatentSBERTa_TAC_claims_results.json\",\n",
    "    \"PatentSBERTa_claims_TAC_results.json\",\n",
    "]\n",
    "base_dir = \"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/github/not_git/best_pipeline_search_results/\"\n",
    "\n",
    "# Load all ranking lists\n",
    "total_result = []\n",
    "for i in best_pipelines:\n",
    "    result = {}\n",
    "    with open(base_dir+i) as f:\n",
    "        data = json.load(f)\n",
    "    for j in data[\"segments\"].keys():\n",
    "        result[j] = list(data[\"segments\"][j].keys())\n",
    "    total_result.append(result)\n",
    "\n",
    "# Apply RRF\n",
    "fused_rankings = reciprocal_rank_fusion(total_result)\n",
    "\n",
    "# Example of how to use or save the results\n",
    "# Save the fused results\n",
    "with open(base_dir + \"fused_results_rrf.json\", \"w\") as f:\n",
    "    json.dump({\"segments\": {query: {doc: idx for idx, doc in enumerate(docs)} \n",
    "                            for query, docs in fused_rankings.items()}}, f, indent=2)\n",
    "\n",
    "# Example: Print top 5 documents for first query\n",
    "sample_query = list(fused_rankings.keys())[0]\n",
    "print(f\"Sample query: {sample_query}\")\n",
    "print(f\"Top 5 documents after fusion: {fused_rankings[sample_query][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted(citing_to_cited_dict, fused_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812018738105695\n",
      "45.987847069731146\n",
      "0.31517117100993475\n"
     ]
    }
   ],
   "source": [
    "recall_at_k = mean_recall_at_k(true_labels, predicted_labels, k=100)\n",
    "print(recall_at_k)\n",
    "mean_rank = mean_ranking(true_labels, predicted_labels)\n",
    "print(mean_rank)\n",
    "\n",
    "mean_inv_rank = mean_inv_ranking(true_labels, predicted_labels)\n",
    "print(mean_inv_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
