{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mean Recall@k for a list of recommendations.\n",
    "    \"\"\"\n",
    "    recalls_at_k = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate Recall@k for each recommendation list\n",
    "        true_set = set(true)\n",
    "        k = min(k, len(pred))\n",
    "        relevant_count = sum(1 for item in pred[:k] if item in true_set)\n",
    "        recalls_at_k.append(relevant_count / len(true_set) if len(true_set) > 0 else 0)\n",
    "\n",
    "    # Calculate the mean Recall@k\n",
    "    mean_recall = sum(recalls_at_k) / len(recalls_at_k) if recalls_at_k else 0\n",
    "\n",
    "    return mean_recall\n",
    "\n",
    "def mean_inv_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean inverse rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = 1 / (pred.index(item) + 1)\n",
    "            except ValueError:\n",
    "                rank = 0  # If item not found, assign 0\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks) if ranks else 0\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean inverse ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else 0\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "def mean_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = pred.index(item) + 1\n",
    "            except ValueError:\n",
    "                rank = len(pred)  # If item not found, assign the length of the list\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks) if ranks else 0\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else 0\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "# for getting true labels and our predictions\n",
    "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
    "    \"\"\"\n",
    "    Get the true and predicted labels for the metrics calculation.\n",
    "    \"\"\"\n",
    "    # for i in recommendations_dict:\n",
    "    #     print(i, recommendations_dict[i])\n",
    "    #     break\n",
    "    \n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    not_in_citation_mapping = 0\n",
    "\n",
    "    # Iterate over the items in both dictionaries\n",
    "    for citing_id in recommendations_dict.keys():\n",
    "        # Check if the citing_id is present in both dictionaries\n",
    "        if citing_id in citing_to_cited_dict:\n",
    "            # If yes, append the recommended items from both dictionaries to the respective lists\n",
    "            true_labels.append(citing_to_cited_dict[citing_id])\n",
    "            predicted_labels.append(recommendations_dict[citing_id])\n",
    "        else:\n",
    "            print(citing_id, \"not in citation mapping\")\n",
    "            not_in_citation_mapping += 1\n",
    "\n",
    "    return true_labels, predicted_labels, not_in_citation_mapping\n",
    "\n",
    "# load embeddings\n",
    "def load_embeddings_and_ids(embedding_file, app_ids_file):\n",
    "    \"\"\"\n",
    "    Load the embeddings and application IDs from saved files\n",
    "    \"\"\"\n",
    "    print(f\"Loading embeddings from {embedding_file}\")\n",
    "    embeddings = torch.from_numpy(np.load(embedding_file))\n",
    "\n",
    "    print(f\"Loading app_ids from {app_ids_file}\")\n",
    "    with open(app_ids_file, 'r') as f:\n",
    "        app_ids = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(embeddings)} embeddings and {len(app_ids)} app_ids\")\n",
    "    return embeddings, app_ids\n",
    "\n",
    "# calculating cosine similarity:\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j] = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "def pytorch_cos_sim(a, b):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j] = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    return cos_sim(a, b)\n",
    "\n",
    "# for getting train alignments\n",
    "def citation_to_citing_to_cited_dict(citations):\n",
    "    \"\"\"\n",
    "    Put a citation mapping in a dict format\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the results\n",
    "    citing_to_cited_dict = {}\n",
    "\n",
    "    # Iterate over the items in the JSON list\n",
    "    for citation in citations:\n",
    "        # Check if the citing id already exists in the resulting dictionary\n",
    "        if citation[0] in citing_to_cited_dict:\n",
    "            # If the citing id exists, append the cited id to the existing list\n",
    "            citing_to_cited_dict[citation[0]].append(citation[2])\n",
    "        else:\n",
    "            # If the citing id doesn't exist, create a new list with the cited id for that citing id\n",
    "            citing_to_cited_dict[citation[0]] = [citation[2]]\n",
    "\n",
    "    return citing_to_cited_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_claims.json\n",
      "Loaded 16834 embeddings and 16834 app_ids\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/embeddings_PatentSBERTa_mean_claims.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/app_ids_PatentSBERTa_mean_claims.json\n",
      "Loaded 1000 embeddings and 1000 app_ids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cffba907a64545ade007d43f1c3bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cosine scores:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 16837 embeddings and 16837 app_ids\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 1000 embeddings and 1000 app_ids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533d936fbe5a4fc3aa8692f3240a4cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cosine scores:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 16837 embeddings and 16837 app_ids\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/embeddings_PatentSBERTa_mean_claims.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/app_ids_PatentSBERTa_mean_claims.json\n",
      "Loaded 1000 embeddings and 1000 app_ids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5423feacb65446cd9fc0a5595316f264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cosine scores:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_claims.json\n",
      "Loaded 16834 embeddings and 16834 app_ids\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 1000 embeddings and 1000 app_ids\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b84b1f709614283a159732890bd1992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cosine scores:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOP_N = 100\n",
    "K_VALUE = 10\n",
    "POOLING = \"mean\"                \n",
    "QUERY_SET = \"test\"  \n",
    "BASE_DIR = \"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025\"\n",
    "DOC_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings/embeddings_precalculated_docs\")\n",
    "TRAIN_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings/embeddings_precalculated_train\")\n",
    "TEST_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings/embeddings_precalculated_test\")\n",
    "OUTPUT_DIR = \"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/github/not_git/best_pipeline_search_results\"\n",
    "CITATION_FILE = os.path.join(BASE_DIR, \"Citation_JSONs/Citation_Train.json\")\n",
    "MODEL_NAME = \"PatentSBERTa\"\n",
    "\n",
    "rankings = []\n",
    "combinations = [\n",
    "    \"claims_claims\",\n",
    "    \"TAC_TAC\",\n",
    "    \"claims_TAC\",\n",
    "    \"TAC_claims\"\n",
    "]\n",
    "\n",
    "for i in combinations:\n",
    "    CONTENT_TYPE_coming, CONTENT_TYPE_existing = i.split(\"_\")\n",
    "    DOC_EMBEDDING_FILE = os.path.join(DOC_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_existing}.npy\")\n",
    "    DOC_APP_IDS_FILE = os.path.join(DOC_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_existing}.json\")\n",
    "\n",
    "    QUERY_EMBEDDING_DIR = TRAIN_EMBEDDING_DIR if QUERY_SET == \"train\" else TEST_EMBEDDING_DIR\n",
    "    QUERY_EMBEDDING_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_coming}.npy\")\n",
    "    QUERY_APP_IDS_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_coming}.json\")\n",
    "\n",
    "    # Load existing embeddings and app_ids\n",
    "    doc_embeddings, doc_app_ids = load_embeddings_and_ids(DOC_EMBEDDING_FILE, DOC_APP_IDS_FILE)\n",
    "\n",
    "    # Load incoming embeddings and app_ids\n",
    "    query_embeddings, query_app_ids = load_embeddings_and_ids(QUERY_EMBEDDING_FILE, QUERY_APP_IDS_FILE)\n",
    "    only_query_results={}\n",
    "    for i, (query_embedding, query_id) in enumerate(tqdm(zip(query_embeddings, query_app_ids), total=len(query_embeddings), desc=\"cosine scores\")):\n",
    "        # Compute cosine similarity\n",
    "        query_embedding = query_embedding.unsqueeze(0)\n",
    "        cos_scores = pytorch_cos_sim(query_embedding, doc_embeddings)[0].cpu()\n",
    "\n",
    "        # Sort results and get top N\n",
    "        top_n_index = torch.argsort(cos_scores, descending=True)[:TOP_N].numpy()\n",
    "\n",
    "        # Get application IDs of top N documents\n",
    "        top_n_app_ids = [doc_app_ids[i] for i in top_n_index]\n",
    "        top_n_scores = cos_scores[top_n_index].tolist()\n",
    "        \n",
    "        #results[query_id][0] = IDS\n",
    "        #results[query_id][0] = scores \n",
    "        only_query_results[query_id] = top_n_app_ids\n",
    "    rankings.append(only_query_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample query: 3692876A1\n",
      "Top 5 documents after fusion: ['1707101B1', '2013126B1', '1925895B1', '3139394A1', '1785083B1']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def reciprocal_rank_fusion(ranked_lists, k=20):\n",
    "    \"\"\"\n",
    "    Implements Reciprocal Rank Fusion to combine multiple ranked lists.\n",
    "    \n",
    "    Args:\n",
    "        ranked_lists: List of dictionaries, where each dictionary maps queries to ranked lists of document IDs\n",
    "        k: Constant to mitigate the impact of high rankings (default: 60)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping queries to fused ranked lists\n",
    "    \"\"\"\n",
    "    fused_results = {}\n",
    "    \n",
    "    # Get all unique query IDs\n",
    "    all_queries = set()\n",
    "    for result_dict in ranked_lists:\n",
    "        all_queries.update(result_dict.keys())\n",
    "    \n",
    "    # Process each query\n",
    "    for query in all_queries:\n",
    "        # Dictionary to store RRF scores for each document\n",
    "        rrf_scores = defaultdict(float)\n",
    "        \n",
    "        # Calculate RRF scores for each document from each ranked list\n",
    "        for result_dict in ranked_lists:\n",
    "            if query in result_dict:\n",
    "                doc_list = result_dict[query]\n",
    "                \n",
    "                # Calculate RRF score based on rank position (1-indexed)\n",
    "                for rank, doc_id in enumerate(doc_list, start=1):\n",
    "                    rrf_scores[doc_id] += 1.0 / (k + rank)\n",
    "        \n",
    "        # Sort documents by RRF score in descending order\n",
    "        sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        fused_results[query] = [doc_id for doc_id, score in sorted_docs]\n",
    "    \n",
    "    return fused_results\n",
    "\n",
    "\n",
    "# Apply RRF\n",
    "fused_rankings = reciprocal_rank_fusion(rankings)\n",
    "\n",
    "# Example of how to use or save the results\n",
    "# Save the fused results\n",
    "with open(\"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/github/not_git/rrf_rankings/\" + \"prediction1.json\", \"w\") as f:\n",
    "    json.dump(fused_rankings, f, indent=2)\n",
    "\n",
    "# Example: Print top 5 documents for first query\n",
    "sample_query = list(fused_rankings.keys())[0]\n",
    "print(f\"Sample query: {sample_query}\")\n",
    "print(f\"Top 5 documents after fusion: {fused_rankings[sample_query][:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
