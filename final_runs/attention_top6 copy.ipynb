{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "def mean_average_precision(true_labels, predicted_labels, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mean Average Precision for a list of recommendations.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels : list of list\n",
    "        True relevant items for each recommendation list.\n",
    "    predicted_labels : list of list\n",
    "        Predicted recommended items for each recommendation list.\n",
    "    k : int\n",
    "        Number of recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean Average Precision value.\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate Average Precision for each recommendation list\n",
    "        true_set = set(true)\n",
    "        precision_at_k = []\n",
    "        relevant_count = 0\n",
    "        for i, item in enumerate(pred[:k]):\n",
    "            if item in true_set:\n",
    "                relevant_count += 1\n",
    "                precision_at_k.append(relevant_count / (i + 1))\n",
    "        average_precision = sum(precision_at_k) / len(true_set)\n",
    "        average_precisions.append(average_precision)\n",
    "\n",
    "    # Calculate the mean Average Precision\n",
    "    mean_average_precision = sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "    return mean_average_precision\n",
    "\n",
    "\n",
    "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mean Recall@k for a list of recommendations.\n",
    "    \"\"\"\n",
    "    recalls_at_k = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate Recall@k for each recommendation list\n",
    "        true_set = set(true)\n",
    "        k = min(k, len(pred))\n",
    "        relevant_count = sum(1 for item in pred[:k] if item in true_set)\n",
    "        recalls_at_k.append(relevant_count / len(true_set) if len(true_set) > 0 else 0)\n",
    "\n",
    "    # Calculate the mean Recall@k\n",
    "    mean_recall = sum(recalls_at_k) / len(recalls_at_k) if recalls_at_k else 0\n",
    "\n",
    "    return mean_recall\n",
    "\n",
    "def mean_inv_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean inverse rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = 1 / (pred.index(item) + 1)\n",
    "            except ValueError:\n",
    "                rank = 0  # If item not found, assign 0\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean inverse rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks) if ranks else 0\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean inverse ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else 0\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "def mean_ranking(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate the mean of lists of the mean rank of true relevant items\n",
    "    in the lists of sorted recommended items.\n",
    "    \"\"\"\n",
    "    mean_ranks = []\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        # Calculate the rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        ranks = []\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = pred.index(item) + 1\n",
    "            except ValueError:\n",
    "                rank = len(pred)  # If item not found, assign the length of the list\n",
    "            ranks.append(rank)\n",
    "\n",
    "        # Calculate the mean rank of true relevant items\n",
    "        # in the recommendation list\n",
    "        mean_rank = sum(ranks) / len(ranks) if ranks else 0\n",
    "        mean_ranks.append(mean_rank)\n",
    "\n",
    "    # Calculate the mean of the mean ranks across all recommendation lists\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else 0\n",
    "\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "# for getting true labels and our predictions\n",
    "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
    "    \"\"\"\n",
    "    Get the true and predicted labels for the metrics calculation.\n",
    "    \"\"\"\n",
    "    # for i in recommendations_dict:\n",
    "    #     print(i, recommendations_dict[i])\n",
    "    #     break\n",
    "    \n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    not_in_citation_mapping = 0\n",
    "\n",
    "    # Iterate over the items in both dictionaries\n",
    "    for citing_id in recommendations_dict.keys():\n",
    "        # Check if the citing_id is present in both dictionaries\n",
    "        if citing_id in citing_to_cited_dict:\n",
    "            # If yes, append the recommended items from both dictionaries to the respective lists\n",
    "            true_labels.append(citing_to_cited_dict[citing_id])\n",
    "            predicted_labels.append(recommendations_dict[citing_id])\n",
    "        else:\n",
    "            print(citing_id, \"not in citation mapping\")\n",
    "            not_in_citation_mapping += 1\n",
    "\n",
    "    return true_labels, predicted_labels, not_in_citation_mapping\n",
    "\n",
    "# load embeddings\n",
    "def load_embeddings_and_ids(embedding_file, app_ids_file):\n",
    "    \"\"\"\n",
    "    Load the embeddings and application IDs from saved files\n",
    "    \"\"\"\n",
    "    print(f\"Loading embeddings from {embedding_file}\")\n",
    "    embeddings = torch.from_numpy(np.load(embedding_file))\n",
    "\n",
    "    print(f\"Loading app_ids from {app_ids_file}\")\n",
    "    with open(app_ids_file, 'r') as f:\n",
    "        app_ids = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(embeddings)} embeddings and {len(app_ids)} app_ids\")\n",
    "    return embeddings, app_ids\n",
    "\n",
    "# calculating cosine similarity:\n",
    "def cos_sim(a, b, normalize):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j] = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    if normalize:\n",
    "        a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "        b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "        return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    else:\n",
    "        return torch.mm(a, b.transpose(0, 1))\n",
    "    \n",
    "\n",
    "def pytorch_cos_sim(a, b, normalize=True):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j] = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    return cos_sim(a, b, normalize)\n",
    "\n",
    "# for getting train alignments\n",
    "def citation_to_citing_to_cited_dict(citations):\n",
    "    \"\"\"\n",
    "    Put a citation mapping in a dict format\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the results\n",
    "    citing_to_cited_dict = {}\n",
    "\n",
    "    # Iterate over the items in the JSON list\n",
    "    for citation in citations:\n",
    "        # Check if the citing id already exists in the resulting dictionary\n",
    "        if citation[0] in citing_to_cited_dict:\n",
    "            # If the citing id exists, append the cited id to the existing list\n",
    "            citing_to_cited_dict[citation[0]].append(citation[2])\n",
    "        else:\n",
    "            # If the citing id doesn't exist, create a new list with the cited id for that citing id\n",
    "            citing_to_cited_dict[citation[0]] = [citation[2]]\n",
    "\n",
    "    return citing_to_cited_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipelines = [\n",
    "    \"claims_claims\",\n",
    "    \"TAC_TAC\",\n",
    "    \"TAC_claims\",\n",
    "    \"claims_TAC\",\n",
    "]\n",
    "## why these were chosen? look at the plot in printing_metrics.ipynb\n",
    "incoming_dtype = [\"claims\",\"TAC\",\"TAC\",\"claims\",\"TA\",\"TA\"]#, \"TAC\", \"TA\", \"claims\"] ## what part of incoming patent to consider\n",
    "existing_dtype = [\"claims\",\"TAC\",\"claims\",\"TAC\", \"TA\", \"TAC\"]#, \"TA\", \"claims\", \"TA\"] ## with what part of existing patent\n",
    "# weigths_patentsbert = [0.81, 0.8, 0.79, 0.79, 0.71, 0.71] ##medians of true scores\n",
    "# weigths_patentsbert = [0.5284088850021362, 0.558128297328949, 0.5326939821243286, 0.5411014556884766, 0.3544950485229492, 0.3110102713108063] ##minimum of true scores\n",
    "\n",
    "TOP_N = 100\n",
    "K_VALUE = 10\n",
    "POOLING = \"mean\"                \n",
    "QUERY_SET = \"train\"                  # for getting the scores, set this to train\n",
    "BASE_DIR = \"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025\"\n",
    "DOC_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings/embeddings_precalculated_docs\")\n",
    "TRAIN_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings/embeddings_precalculated_train\")\n",
    "TEST_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings/embeddings_precalculated_test\")\n",
    "OUTPUT_DIR = \"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/github/not_git/best_pipeline_search_results\"\n",
    "CITATION_FILE = os.path.join(BASE_DIR, \"Citation_JSONs/Citation_Train.json\")\n",
    "MODEL_NAME = \"PatentSBERTa\"\n",
    "\n",
    "with open(CITATION_FILE, 'r') as f:\n",
    "    citations = json.load(f)\n",
    "citing_to_cited_dict = citation_to_citing_to_cited_dict(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/embeddings_PatentSBERTa_mean_claims.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/app_ids_PatentSBERTa_mean_claims.json\n",
      "Loaded 6831 embeddings and 6831 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 6831 embeddings and 6831 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 6831 embeddings and 6831 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/embeddings_PatentSBERTa_mean_claims.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/app_ids_PatentSBERTa_mean_claims.json\n",
      "Loaded 6831 embeddings and 6831 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/embeddings_PatentSBERTa_mean_TA.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/app_ids_PatentSBERTa_mean_TA.json\n",
      "Loaded 6831 embeddings and 6831 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/embeddings_PatentSBERTa_mean_TA.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_train/app_ids_PatentSBERTa_mean_TA.json\n",
      "Loaded 6831 embeddings and 6831 app_ids\n"
     ]
    }
   ],
   "source": [
    "## first get the embeddings from patentBERT for queries\n",
    "incoming_id2emb = {}\n",
    "for CONTENT_TYPE_coming in incoming_dtype:\n",
    "    print()\n",
    "    QUERY_EMBEDDING_DIR = TRAIN_EMBEDDING_DIR if QUERY_SET == \"train\" else TEST_EMBEDDING_DIR\n",
    "    QUERY_EMBEDDING_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_coming}.npy\")\n",
    "    QUERY_APP_IDS_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_coming}.json\")\n",
    "    query_embeddings, query_app_ids = load_embeddings_and_ids(QUERY_EMBEDDING_FILE, QUERY_APP_IDS_FILE)\n",
    "    for i in query_app_ids:\n",
    "        try:\n",
    "            incoming_id2emb[i].append(query_embeddings[query_app_ids.index(i)])\n",
    "        except:\n",
    "            incoming_id2emb[i] = [query_embeddings[query_app_ids.index(i)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_claims.json\n",
      "Loaded 16834 embeddings and 16834 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 16837 embeddings and 16837 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_claims.json\n",
      "Loaded 16834 embeddings and 16834 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 16837 embeddings and 16837 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TA.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TA.json\n",
      "Loaded 16837 embeddings and 16837 app_ids\n",
      "\n",
      "Loading embeddings from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TAC.npy\n",
      "Loading app_ids from /Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TAC.json\n",
      "Loaded 16837 embeddings and 16837 app_ids\n"
     ]
    }
   ],
   "source": [
    "## first get the embeddings from patentBERT for exisiting patents\n",
    "\n",
    "existing_id2emb = {}\n",
    "for CONTENT_TYPE_existing in existing_dtype:\n",
    "    print()\n",
    "    DOC_EMBEDDING_FILE = os.path.join(DOC_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_existing}.npy\")\n",
    "    DOC_APP_IDS_FILE = os.path.join(DOC_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_existing}.json\")\n",
    "\n",
    "    doc_embeddings, doc_app_ids = load_embeddings_and_ids(DOC_EMBEDDING_FILE, DOC_APP_IDS_FILE)\n",
    "    for i in doc_app_ids:\n",
    "        try:\n",
    "            existing_id2emb[i].append(doc_embeddings[doc_app_ids.index(i)])\n",
    "        except:\n",
    "            existing_id2emb[i] = [doc_embeddings[doc_app_ids.index(i)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## do the same thing for the best combination of incoming patent and existing patent for all-MiniLM...\n",
    "\n",
    "# incoming_dtype = [\"TAC\", \"claims\", \"claims\", \"TAC\"] ## what part of incoming patent to comapre with\n",
    "# existing_dtype = [\"TAC\", \"claims\", \"TAC\", \"claims\"] ## what part of existing patent\n",
    "\n",
    "# ## weights according to min true_segments\n",
    "# # weights_minilm = [0.28662535548210144, 0.2773967683315277, 0.29081717133522034]\n",
    "\n",
    "# # weights = weigths_patentsbert + weigths_patentsbert\n",
    "# # weights = np.array(weights)/np.sum()\n",
    "# MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# for CONTENT_TYPE_coming in incoming_dtype:\n",
    "#     QUERY_EMBEDDING_DIR = TRAIN_EMBEDDING_DIR if QUERY_SET == \"train\" else TEST_EMBEDDING_DIR\n",
    "#     QUERY_EMBEDDING_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_coming}.npy\")\n",
    "#     QUERY_APP_IDS_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_coming}.json\")\n",
    "#     query_embeddings, query_app_ids = load_embeddings_and_ids(QUERY_EMBEDDING_FILE, QUERY_APP_IDS_FILE)\n",
    "#     for i in query_app_ids:\n",
    "#         try:\n",
    "#             incoming_id2emb[i].append(query_embeddings[query_app_ids.index(i)])\n",
    "#         except:\n",
    "#             incoming_id2emb[i] = [query_embeddings[query_app_ids.index(i)]]\n",
    "\n",
    "# for CONTENT_TYPE_existing in existing_dtype:\n",
    "#     DOC_EMBEDDING_FILE = os.path.join(DOC_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_existing}.npy\")\n",
    "#     DOC_APP_IDS_FILE = os.path.join(DOC_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_existing}.json\")\n",
    "\n",
    "#     doc_embeddings, doc_app_ids = load_embeddings_and_ids(DOC_EMBEDDING_FILE, DOC_APP_IDS_FILE)\n",
    "#     for i in doc_app_ids:\n",
    "#         try:\n",
    "#             existing_id2emb[i].append(doc_embeddings[doc_app_ids.index(i)])\n",
    "#         except:\n",
    "#             existing_id2emb[i] = [doc_embeddings[doc_app_ids.index(i)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.isfile(\"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/CodaBench/IR2025/embeddings/embeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_claim.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_pool(embeddings):\n",
    "\n",
    "    query = torch.mean(embeddings, dim=0, keepdim=True)  # Context vector -- mean\n",
    "    scores = torch.matmul(embeddings, query.T).squeeze()\n",
    "    weights = torch.softmax(scores, dim=0)\n",
    "    return torch.sum(weights.unsqueeze(-1) * embeddings, dim=0)\n",
    "\n",
    "def pooling(data_dict, operation, num_embs): #num_embs = total number of embeddings for incoming/exisiting patent\n",
    "\n",
    "    print(f\"length on input dict: {len(data_dict)}\")\n",
    "\n",
    "    if operation==\"concatenation\":\n",
    "        keys_to_delete=[]\n",
    "        for i in data_dict:\n",
    "            if len(data_dict[i])==num_embs:\n",
    "                data_dict[i] = torch.cat(data_dict[i])\n",
    "                # print(data_dict[i].shape)\n",
    "            else:\n",
    "                keys_to_delete.append(i)\n",
    "        for key in keys_to_delete:\n",
    "            del data_dict[key]\n",
    "            \n",
    "    if operation==\"average\":\n",
    "        keys_to_delete=[]\n",
    "        for i in data_dict:\n",
    "            if len(data_dict[i])==num_embs:\n",
    "                data_dict[i] = torch.mean(torch.stack(data_dict[i], dim=0), dim=0)\n",
    "            else:\n",
    "                keys_to_delete.append(i)\n",
    "        for key in keys_to_delete:\n",
    "            del data_dict[key]\n",
    "    \n",
    "    if operation==\"addition\":\n",
    "        keys_to_delete=[]\n",
    "        for i in data_dict:\n",
    "            if len(data_dict[i])==num_embs:\n",
    "                data_dict[i] = torch.mean(torch.stack(data_dict[i], dim=0), dim=0)*num_embs\n",
    "            else:\n",
    "                keys_to_delete.append(i)\n",
    "        for key in keys_to_delete:\n",
    "            del data_dict[key]\n",
    "    \n",
    "    if operation == \"attention\":\n",
    "        keys_to_delete=[]\n",
    "        for i in data_dict:\n",
    "            if len(data_dict[i])==num_embs:\n",
    "                data_dict[i] = attention_pool(torch.stack(data_dict[i]))\n",
    "            else:\n",
    "                keys_to_delete.append(i)\n",
    "        for key in keys_to_delete:\n",
    "            del data_dict[key]\n",
    "\n",
    "    print(f\"length on output dict: {len(data_dict)}\")\n",
    "    return data_dict\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length on input dict: 6831\n",
      "length on output dict: 6831\n",
      "\n",
      "length on input dict: 16834\n",
      "length on output dict: 16828\n"
     ]
    }
   ],
   "source": [
    "incoming_id2emb = pooling(incoming_id2emb, \"attention\", num_embs=6)\n",
    "print()\n",
    "existing_id2emb = pooling(existing_id2emb, \"attention\", num_embs=6)\n",
    "# incoming_id2emb_minilm = pooling(incoming_id2emb_minilm, \"multi_strategy\", num_embs=4)\n",
    "# existing_id2emb_minilm = pooling(existing_id2emb_minilm, \"multi_strategy\", num_embs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6831, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings, query_ids = list(incoming_id2emb.values()), list(incoming_id2emb.keys())\n",
    "# print(query_embeddings[0])\n",
    "query_embeddings = torch.stack(query_embeddings)\n",
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16828, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_embeddings, doc_ids = list(existing_id2emb.values()), list(existing_id2emb.keys())\n",
    "doc_embeddings = torch.stack(doc_embeddings)\n",
    "doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10ae18984284588afbd428c28e5d872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cosine scores:   0%|          | 0/6831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "only_query_results = {}\n",
    "for i, (query_embedding, query_id) in enumerate(tqdm(zip(query_embeddings, query_app_ids), total=len(query_embeddings), desc=\"cosine scores\")):\n",
    "    # Compute cosine similarity\n",
    "    query_embedding = query_embedding.unsqueeze(0)\n",
    "    cos_scores = pytorch_cos_sim(query_embedding, doc_embeddings, normalize=True)[0].cpu()\n",
    "\n",
    "    # Sort results and get top N\n",
    "    top_n_index = torch.argsort(cos_scores, descending=True)[:TOP_N].numpy()\n",
    "\n",
    "    # Get application IDs of top N documents\n",
    "    top_n_app_ids = [doc_app_ids[i] for i in top_n_index]\n",
    "    top_n_scores = cos_scores[top_n_index].tolist()\n",
    "    \n",
    "    #results[query_id][0] = IDS\n",
    "    #results[query_id][0] = scores \n",
    "    only_query_results[query_id] = top_n_app_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (\"/Users/kshitij/Documents/UPSaclay/T4/InfoRetrieval/github/not_git/concatenate_13/prediction1.json\",\"w\") as f:\n",
    "#     json.dump(only_query_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5659078709803346\n",
      "28.82070463084957\n",
      "0.3556313922052502\n",
      "0.36852854188996453\n"
     ]
    }
   ],
   "source": [
    "true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted(citing_to_cited_dict, only_query_results)\n",
    "\n",
    "recall_at_k = mean_recall_at_k(true_labels, predicted_labels, k=K_VALUE)\n",
    "print(recall_at_k)\n",
    "mean_rank = mean_ranking(true_labels, predicted_labels)\n",
    "print(mean_rank)\n",
    "\n",
    "mean_inv_rank = mean_inv_ranking(true_labels, predicted_labels)\n",
    "print(mean_inv_rank)\n",
    "\n",
    "map = []\n",
    "for i in range(10,100,10):\n",
    "    map.append(mean_average_precision(true_labels, predicted_labels, k=i))\n",
    "print(np.mean(map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconcatenate | norm = True | first 6 docs + 3 lm doc\\n0.6046357292734102\\n24.858563899868248\\n0.38852029919424486\\n0.40331076676419736\\n\\n0.37 vs 0.38 normalize false vs true for avg of patentsberta\\n\\nattention pooling on patentsberta -- 0.3567296318474875\\n\\nmixed -- 0.39623929734429403 -- avg concatenated with max pool\\n\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "addition | norm = True | first 4 docs\n",
    "0.5443639291465378\n",
    "30.718111062313977\n",
    "0.3435209450449923\n",
    "0.35557668046845436\n",
    "'''\n",
    "\n",
    "'''\n",
    "addition | norm = True | first 6 docs\n",
    "0.5685990338164252\n",
    "28.041257990533353\n",
    "0.3607615251149155\n",
    "0.3739892150148598\n",
    "'''\n",
    "\n",
    "'''\n",
    "average | norm = True | first 6 docs\n",
    "0.5685990338164252\n",
    "28.041209193383107\n",
    "0.36076161351555003\n",
    "0.373989421283007\n",
    "'''\n",
    "\n",
    "'''\n",
    "concatenate | norm = True | first 6 docs + 1 lm doc\n",
    "0.5971941638608307\n",
    "25.415602888791298\n",
    "0.3824726140500106\n",
    "0.39687537978272747\n",
    "'''\n",
    "\n",
    "'''\n",
    "concatenate | norm = True | first 6 docs + 3 lm doc\n",
    "0.6046357292734102\n",
    "24.858563899868248\n",
    "0.38852029919424486\n",
    "0.40331076676419736\n",
    "\n",
    "0.37 vs 0.38 normalize false vs true for avg of patentsberta\n",
    "\n",
    "attention pooling on patentsberta -- 0.3567296318474875\n",
    "\n",
    "mixed -- 0.39623929734429403 -- avg concatenated with max pool\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
